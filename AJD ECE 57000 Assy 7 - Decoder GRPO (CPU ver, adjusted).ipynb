{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bVhoyckM5w7MSspIF7ZFMDCjjCD2dEJy","timestamp":1764991566049}],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNtLxIwdxlSwWmmyHVsgAC/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","format:\n","html:\n","code-fold: true\n","jupyter: python3\n","---"],"metadata":{"id":"U1h96Nj9ksB8"}},{"cell_type":"markdown","source":["# Cell 1: Task Definition and Plan\n","\n","In this experiment, I will fine-tune a **tiny decoder-only language model** with **Group Relative Policy Optimization (GRPO)** on a simple, fully synthetic reasoning task based on 5-card poker hands.\n","\n","Due to [a persistent CUDA device-side assert crash with GPU runtimes](https://github.com/googlecolab/colabtools/issues/5749) (attempted with both A100 and T4 GPUs), I developed this program on a **CPU** runtime. The \"High-RAM\" runtime setting is also recommended: my test runs have never entirely run out of memory, but the program has come close to maxing out the default RAM size on some runs.\n","\n","---\n","\n","## Tiny Model\n","\n","- **Model**: `HuggingFaceTB/SmolLM2-135M`\n","- **Type**: Pretrained **decoder-only** causal language model (small ~135M parameter variant of SmolLM2).\n","- **Usage**:\n","  - One copy as the **policy model** to be updated with GRPO.\n","  - A frozen copy as an optional **reference model** for KL regularization.\n","\n","---\n","\n","## Toy Task: Classifying 5-Card Poker Hands\n","\n","In standard 5-card draw poker with a 52-card deck (no jokers), there are **2,598,960** distinct hands.  \n","For each randomly sampled hand, the model’s job is to classify the **hand type**.\n","\n","### Prompt Structure\n","\n","- Cards are represented as `RANKSUIT`, e.g. `2H` (2 of Hearts), `TD` (Ten of Diamonds), `AS` (Ace of Spades).\n","- The prompt is natural language plus the hand, followed by a constrained answer instruction.\n","\n","**Example prompt:**\n","\n","> Classify this 5-card poker hand.  \n",">  \n","> Hand: 2H 2D 2C 9S 9H  \n","> Question: What type of hand is this? Answer with one of:  \n","> high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.  \n",">  \n","> Answer:\n","\n","### Desired Output\n","\n","One of the following **eight labels** (verbatim):\n","\n","- `high card` - No pattern (over half of all possible hands). Reward weight: `1.0`\n","- `one pair` - Two cards of the same rank. Reward weight: `1.0`\n","- `two pair` - One pair each of two ranks. Reward weight: `1.5`\n","- `three of a kind` - Three cards of the same rank. Reward weight: `2.0`\n","- `straight` - Five cards of consecutive ranks. Reward weight: `2.0`\n","- `flush` - Five cards of the same suit. Reward weight: `2.0`.\n","- `full house` - Three of a kind + one pair. Reward weight: `2.5`\n","- `four of a kind` - Four cards of the same rank. Reward weight: `3.0`.\n","\n","The model generates a short textual answer, ideally matching exactly one of these labels. In the example prompt, we have three 2s and two 9s, so the correct answer is `full house` (specifically \"twos full of nines\", but we aren't having it classify that deeply).\n","\n","---\n","\n","## Reward Logic\n","\n","A simple rule-based **oracle** will compute the correct hand type from the cards.\n","\n","For each sampled completion:\n","\n","1. Parse the model’s answer (lowercase, strip whitespace/punctuation).\n","2. Compare to the oracle’s label.\n","\n","**Reward:**\n","\n","- Weighted reward as above if the model’s answer matches the oracle’s label exactly.\n","- `0.0` otherwise.\n","\n","Optionally, a **KL penalty** term can be added to the reward:\n","- Estimate token-level KL divergence between the policy model and the frozen reference model on the generated answer span.\n","- Use a **shaped reward**:  \n","  $$\n","  r = r_{\\text{task}} - \\beta \\cdot \\text{KL}(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}})\n","  $$\n","\n","---\n","\n","## Hypothesis\n","\n","- Before GRPO fine-tuning, the tiny model will often produce **incorrect or inconsistent** hand-type labels, despite fluent English.\n","- After GRPO:\n","  - The model should **increase its accuracy** on held-out test hands, learning the underlying **combinatorial patterns** of 5-card hands (e.g., recognition of pairs, flushes, and full houses).\n","  - Generated answers should more reliably be one of the **eight allowed labels** rather than arbitrary phrases.\n","- If the KL term is used, I expect:\n","  - The model to **improve task performance** while remaining relatively close to the original pretrained behavior, avoiding degenerate outputs or mode collapse on a single label.\n"],"metadata":{"id":"BW2HWJ_6xdsH"}},{"cell_type":"code","source":["# Cell 2: Setup and Model Loading (CPU-only, stable)\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# ---------------------------------------------\n","# Configuration\n","# ---------------------------------------------\n","model_name = \"HuggingFaceTB/SmolLM2-135M\"\n","\n","# Force CPU to avoid CUDA device-side asserts\n","device = torch.device(\"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ---------------------------------------------\n","# Load tokenizer\n","# ---------------------------------------------\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Ensure correct padding for decoder-only models\n","tokenizer.padding_side = \"left\"\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","print(\"Tokenizer pad_token_id:\", tokenizer.pad_token_id)\n","print(\"Tokenizer eos_token_id:\", tokenizer.eos_token_id)\n","\n","# ---------------------------------------------\n","# Load model on CPU in float32\n","# ---------------------------------------------\n","dtype = torch.float32\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype=dtype,\n",")\n","model.to(device)\n","\n","# Frozen reference model for optional KL (kept for completeness)\n","reference_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype=dtype,\n",")\n","reference_model.to(device)\n","reference_model.eval()\n","\n","print(\"\\nModel Architecture:\")\n","print(model)\n","\n","num_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\nTotal Parameters: {num_params:,}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5LtdwnCHFSJ","executionInfo":{"status":"ok","timestamp":1767347237929,"user_tz":300,"elapsed":4893,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"dcf009c1-bcc4-4c48-abe5-a676b7271d80"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Tokenizer pad_token_id: 0\n","Tokenizer eos_token_id: 0\n","\n","Model Architecture:\n","LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(49152, 576)\n","    (layers): ModuleList(\n","      (0-29): 30 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n","          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n","          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n","          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n","          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n","          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((576,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",")\n","\n","Total Parameters: 134,515,008\n"]}]},{"cell_type":"code","source":["# Cell 3: Balanced Dataset Generation (by hand type)\n","\n","import random\n","\n","# ---------------------------------------------\n","# Card deck construction\n","# ---------------------------------------------\n","RANKS = [\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"T\",\"J\",\"Q\",\"K\",\"A\"]\n","SUITS = [\"H\",\"D\",\"C\",\"S\"]   # Hearts, Diamonds, Clubs, Spades\n","\n","DECK = [r+s for r in RANKS for s in SUITS]\n","\n","# ---------------------------------------------\n","# Oracle: classify a 5-card poker hand\n","# ---------------------------------------------\n","def hand_to_ranks(hand):\n","    \"\"\"Return sorted list of ranks mapped to numeric values.\"\"\"\n","    rank_order = {r:i for i,r in enumerate(RANKS)}  # 2→0, ..., A→12\n","    return sorted([rank_order[c[0]] for c in hand])\n","\n","def is_flush(hand):\n","    suits = [c[1] for c in hand]\n","    return len(set(suits)) == 1\n","\n","def is_straight(ranks):\n","    return all(ranks[i] + 1 == ranks[i+1] for i in range(4))\n","\n","def classify_hand(hand):\n","    \"\"\"\n","    Return one of:\n","    high card, one pair, two pair, three of a kind,\n","    straight, flush, full house, four of a kind\n","    \"\"\"\n","    ranks = hand_to_ranks(hand)\n","    flush = is_flush(hand)\n","    straight = is_straight(ranks)\n","\n","    # Count occurrences of each rank\n","    counts = {}\n","    for c in ranks:\n","        counts[c] = counts.get(c, 0) + 1\n","    freq = sorted(counts.values(), reverse=True)\n","\n","    if flush and straight:\n","        # For simplicity, treat straight flush as \"straight\"\n","        return \"straight\"\n","    if freq == [4,1]:\n","        return \"four of a kind\"\n","    if freq == [3,2]:\n","        return \"full house\"\n","    if flush:\n","        return \"flush\"\n","    if straight:\n","        return \"straight\"\n","    if freq == [3,1,1]:\n","        return \"three of a kind\"\n","    if freq == [2,2,1]:\n","        return \"two pair\"\n","    if freq == [2,1,1,1]:\n","        return \"one pair\"\n","    return \"high card\"\n","\n","# ---------------------------------------------\n","# Prompt template\n","# ---------------------------------------------\n","def build_prompt(hand):\n","    cards = \" \".join(hand)\n","    return (\n","        \"Classify this 5-card poker hand.\\n\\n\"\n","        f\"Hand: {cards}\\n\"\n","        \"Question: What type of hand is this? Answer with exactly one of:\\n\"\n","        \"high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\\n\"\n","        \"Do not explain your answer.\\n\\n\"\n","        \"Answer:\"\n","    )\n","\n","# ---------------------------------------------\n","# Balanced dataset generator\n","# ---------------------------------------------\n","HAND_TYPES = [\n","    \"high card\",\n","    \"one pair\",\n","    \"two pair\",\n","    \"three of a kind\",\n","    \"straight\",\n","    \"flush\",\n","    \"full house\",\n","    \"four of a kind\",\n","]\n","\n","def generate_balanced_dataset(n_per_class=300, max_attempts=1_000_000):\n","    \"\"\"\n","    Generate a dataset with (approximately) n_per_class examples\n","    for each hand type, using rejection sampling.\n","    \"\"\"\n","    counts = {t: 0 for t in HAND_TYPES}\n","    dataset = []\n","    attempts = 0\n","\n","    while min(counts.values()) < n_per_class and attempts < max_attempts:\n","        attempts += 1\n","        hand = random.sample(DECK, 5)\n","        label = classify_hand(hand)\n","        if label not in counts:\n","            continue\n","        if counts[label] >= n_per_class:\n","            continue\n","\n","        prompt = build_prompt(hand)\n","        dataset.append({\"prompt\": prompt, \"label\": label})\n","        counts[label] += 1\n","\n","    print(\"Balanced dataset counts:\", counts)\n","    print(\"Total hands sampled:\", attempts)\n","    return dataset\n","\n","# Create training and evaluation sets\n","train_data = generate_balanced_dataset(n_per_class=300)  # ~2400 examples\n","eval_data  = generate_balanced_dataset(n_per_class=50)   # ~400 examples\n","\n","# Show a few examples\n","print(\"\\nExample prompts from balanced training data:\\n\")\n","for i in range(3):\n","    print(f\"--- Example {i+1} ---\")\n","    print(train_data[i][\"prompt\"])\n","    print(\"Correct label:\", train_data[i][\"label\"])\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_KMS7dwXtsy","executionInfo":{"status":"ok","timestamp":1767347248340,"user_tz":300,"elapsed":10402,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"a145b7e5-8391-4a36-a843-a231276ef295"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Balanced dataset counts: {'high card': 300, 'one pair': 300, 'two pair': 300, 'three of a kind': 300, 'straight': 300, 'flush': 300, 'full house': 300, 'four of a kind': 238}\n","Total hands sampled: 1000000\n","Balanced dataset counts: {'high card': 50, 'one pair': 50, 'two pair': 50, 'three of a kind': 50, 'straight': 50, 'flush': 50, 'full house': 50, 'four of a kind': 50}\n","Total hands sampled: 243029\n","\n","Example prompts from balanced training data:\n","\n","--- Example 1 ---\n","Classify this 5-card poker hand.\n","\n","Hand: TH TS 5S QS KD\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","Correct label: one pair\n","\n","--- Example 2 ---\n","Classify this 5-card poker hand.\n","\n","Hand: 3D 7D 6S 3S 8H\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","Correct label: one pair\n","\n","--- Example 3 ---\n","Classify this 5-card poker hand.\n","\n","Hand: 3C 6S TS 8C TH\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","Correct label: one pair\n","\n"]}]},{"cell_type":"code","source":["# Cell 4: Reward Function with Class-Weighted Shaping\n","\n","import re\n","import torch\n","\n","# Allowed labels (verbatim, lowercase)\n","VALID_LABELS = [\n","    \"high card\",\n","    \"one pair\",\n","    \"two pair\",\n","    \"three of a kind\",\n","    \"straight\",\n","    \"flush\",\n","    \"full house\",\n","    \"four of a kind\"\n","]\n","\n","# Class weights: encourage learning of more structured hands\n","CLASS_WEIGHTS = {\n","    \"high card\":      1.0,\n","    \"one pair\":       1.0,\n","    \"two pair\":       1.5,\n","    \"three of a kind\":2.0,\n","    \"straight\":       2.0,\n","    \"flush\":          2.0,\n","    \"full house\":     2.5,\n","    \"four of a kind\": 3.0,\n","}\n","\n","def extract_label(response: str):\n","    text = response.strip().lower()\n","    text = text.replace(\":\", \"\").replace(\".\", \"\").strip()\n","    return text if text in VALID_LABELS else None\n","\n","def get_reward(prompts, responses, labels):\n","    \"\"\"\n","    Compute reward for a batch of (prompt, response, label).\n","\n","    Inputs:\n","      prompts:   list of prompt strings  (unused but kept for API consistency)\n","      responses: list of model response strings\n","      labels:    list of ground-truth labels (oracle output)\n","\n","    Output:\n","      torch.tensor of rewards (float32), shape (batch,)\n","    \"\"\"\n","    rewards = []\n","    for resp, gold in zip(responses, labels):\n","        pred = extract_label(resp)\n","        if pred == gold and gold in CLASS_WEIGHTS:\n","            # Correct answer: reward is scaled by class weight\n","            rewards.append(CLASS_WEIGHTS[gold])\n","        else:\n","            # Incorrect or unusable answer\n","            rewards.append(0.0)\n","    return torch.tensor(rewards, dtype=torch.float32)\n","\n","\n","# -------------------------------------------------------\n","# Sanity Check\n","# -------------------------------------------------------\n","\n","test_prompts = [\n","    \"Classify this hand: 2H 2D 2C 9S 9H\",\n","    \"Classify this hand: AH 7D 5C 3S 2H\"\n","]\n","\n","test_labels = [\"full house\", \"high card\"]\n","\n","test_responses = [\n","    \"full house\",      # correct, higher-weight class\n","    \"high card\",       # correct, baseline weight\n","]\n","\n","test_rewards = get_reward(test_prompts, test_responses, test_labels)\n","\n","print(\"Sanity-Check of Reward Function:\\n\")\n","for p, r, gold, rew in zip(test_prompts, test_responses, test_labels, test_rewards):\n","    print(\"Prompt:   \", p)\n","    print(\"Response: \", r)\n","    print(\"Gold:     \", gold)\n","    print(\"Reward:   \", float(rew))\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loVbyIMwZOqV","executionInfo":{"status":"ok","timestamp":1767347248355,"user_tz":300,"elapsed":7,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"816a1a9b-4986-4b48-8c2a-cf6cf08c62dc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Sanity-Check of Reward Function:\n","\n","Prompt:    Classify this hand: 2H 2D 2C 9S 9H\n","Response:  full house\n","Gold:      full house\n","Reward:    2.5\n","\n","Prompt:    Classify this hand: AH 7D 5C 3S 2H\n","Response:  high card\n","Gold:      high card\n","Reward:    1.0\n","\n"]}]},{"cell_type":"code","source":["# Cell 5: Single GRPO Step (definition + sanity test)\n","\n","import torch\n","\n","# Make sure padding is correct\n","tokenizer.padding_side = \"left\"\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","VALID_LABELS = [\n","    \"high card\",\n","    \"one pair\",\n","    \"two pair\",\n","    \"three of a kind\",\n","    \"straight\",\n","    \"flush\",\n","    \"full house\",\n","    \"four of a kind\",\n","]\n","\n","# Tokenize labels without special tokens\n","label_token_ids = [\n","    tokenizer(label, add_special_tokens=False).input_ids\n","    for label in VALID_LABELS\n","]\n","\n","# Optional: allow a leading space (often needed for BPE tokenizers)\n","label_token_ids_spaced = [\n","    tokenizer(\" \" + label, add_special_tokens=False).input_ids\n","    for label in VALID_LABELS\n","]\n","\n","# Combine both variants (some models prefer one)\n","ALL_LABEL_SEQS = label_token_ids + label_token_ids_spaced\n","\n","# Calculate the maximum length of any allowed label sequence once\n","MAX_LABEL_SEQ_LEN = max(len(seq) for seq in ALL_LABEL_SEQS)\n","\n","\n","def make_prefix_allowed_tokens_fn(input_len, allowed_label_seqs, eos_id, max_label_seq_len):\n","    \"\"\"\n","    Returns a prefix_allowed_tokens_fn closure bound to this batch.\n","    input_len: padded input length at generation start (int)\n","    \"\"\"\n","    def prefix_allowed_tokens_fn(batch_id, input_ids):\n","        # tokens generated beyond initial padded input\n","        gen_len = input_ids.shape[0] - input_len\n","\n","        if gen_len < 0:\n","            return [eos_id]\n","\n","        candidates = set()\n","        if gen_len < max_label_seq_len:\n","            for seq in allowed_label_seqs:\n","                if gen_len < len(seq):\n","                    candidates.add(seq[gen_len])\n","                elif gen_len == len(seq):\n","                    candidates.add(eos_id)\n","        else:\n","            candidates.add(eos_id)\n","\n","        return list(candidates)\n","\n","    return prefix_allowed_tokens_fn\n","\n","\n","def grpo_step(\n","    prompts,\n","    labels,\n","    G=2,\n","    max_new_tokens=MAX_LABEL_SEQ_LEN + 1,\n","    beta_kl=0.0,\n","    use_kl=False,\n","    entropy_coef=0.0,   # set to e.g. 0.01 if you want to reduce mode collapse\n","):\n","    \"\"\"\n","    Perform a single GRPO step (forward pass only).\n","\n","    Returns:\n","      loss: scalar tensor\n","      advantages: (B*G,) tensor\n","      task_rewards: (B*G,) tensor\n","      kl_seq: (B*G,) tensor\n","    \"\"\"\n","    device = next(model.parameters()).device\n","    batch_size = len(prompts)\n","\n","    # 1) Expand prompts so each appears G times\n","    expanded_prompts = [p for p in prompts for _ in range(G)]\n","    expanded_labels  = [lab for lab in labels  for _ in range(G)]\n","\n","    enc = tokenizer(\n","        expanded_prompts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True\n","    ).to(device)\n","\n","    input_ids = enc[\"input_ids\"]\n","    attention_mask = enc[\"attention_mask\"]\n","\n","    # IMPORTANT: use padded input length as the prompt boundary (works with left padding)\n","    input_len = input_ids.shape[1]\n","\n","    # 2) Constrained generation\n","    prefix_fn = make_prefix_allowed_tokens_fn(\n","        input_len=input_len,\n","        allowed_label_seqs=ALL_LABEL_SEQS,\n","        eos_id=tokenizer.eos_token_id,\n","        max_label_seq_len=MAX_LABEL_SEQ_LEN,\n","    )\n","\n","    with torch.inference_mode():\n","        gen_outputs = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=True,\n","            top_k=0,\n","            top_p=1.0,\n","            prefix_allowed_tokens_fn=prefix_fn,\n","            pad_token_id=tokenizer.pad_token_id,\n","            eos_token_id=tokenizer.eos_token_id,\n","            return_dict_in_generate=True,\n","        )\n","\n","    # Clone and detach generated sequences and attention mask.\n","    # This converts them from inference tensors to regular tensors,\n","    # allowing them to be used in subsequent model calls for log-prob calculation.\n","    gen_sequences = gen_outputs.sequences.clone().detach() # (B*G, input_len + gen_len)\n","    gen_attn_mask = (gen_sequences != tokenizer.pad_token_id).to(device).clone().detach()\n","\n","    # 3) Log-probs of generated tokens\n","    outputs = model(gen_sequences, attention_mask=gen_attn_mask)\n","    logits = outputs.logits  # (B*G, L, V)\n","\n","    logprobs = torch.log_softmax(logits, dim=-1)\n","    targets = gen_sequences[:, 1:]  # (B*G, L-1)\n","    logprobs_tokens = logprobs[:, :-1, :].gather(\n","        dim=-1,\n","        index=targets.unsqueeze(-1)\n","    ).squeeze(-1)\n","\n","    # Mask ONLY generated tokens (exclude the padded prompt region)\n","    # The first generated token is at position index = input_len (0-based) in gen_sequences\n","    # In `targets` (shifted by 1), that corresponds to index = input_len - 1\n","    seq_len_minus1 = targets.size(1)\n","    gen_token_mask = torch.zeros(\n","        (gen_sequences.size(0), seq_len_minus1),\n","        dtype=torch.bool,\n","        device=device\n","    )\n","\n","    start = max(input_len - 1, 0)\n","    gen_token_mask[:, start:] = True\n","\n","    tgt_pad_mask = (targets != tokenizer.pad_token_id)\n","    final_mask = gen_token_mask & tgt_pad_mask  # (B*G, L-1)\n","\n","    logprob_seq = (logprobs_tokens * final_mask).sum(dim=1)  # (B*G,)\n","\n","    # 4) Decode ONLY the generated completion using input_len boundary\n","    completions = []\n","    for seq in gen_sequences: # Use the cloned gen_sequences here\n","        completion_ids = seq[input_len:]  # tokens after padded input\n","        text = tokenizer.decode(completion_ids, skip_special_tokens=True)\n","        completions.append(text)\n","\n","    task_rewards = get_reward(expanded_prompts, completions, expanded_labels).to(device)\n","\n","    # 5) Optional KL vs reference_model (still works if you want it)\n","    if use_kl:\n","        with torch.no_grad():\n","            ref_outputs = reference_model(gen_sequences, attention_mask=gen_attn_mask) # Use cloned tensors here\n","            ref_logits = ref_outputs.logits\n","            ref_logprobs = torch.log_softmax(ref_logits, dim=-1)\n","            ref_logprobs_tokens = ref_logprobs[:, :-1, :].gather(\n","                dim=-1,\n","                index=targets.unsqueeze(-1)\n","            ).squeeze(-1)\n","\n","        log_ratio = (logprobs_tokens - ref_logprobs_tokens) * final_mask\n","        kl_seq = log_ratio.sum(dim=1) / final_mask.sum(dim=1).clamp_min(1)\n","    else:\n","        kl_seq = torch.zeros_like(logprob_seq)\n","\n","    shaped_rewards = task_rewards - beta_kl * kl_seq  # (B*G,)\n","\n","    # 6) Group-wise normalized advantages\n","    shaped_rewards_group = shaped_rewards.view(batch_size, G)\n","    group_mean = shaped_rewards_group.mean(dim=1, keepdim=True)\n","    group_std = shaped_rewards_group.std(dim=1, keepdim=True) + 1e-8\n","    advantages_group = (shaped_rewards_group - group_mean) / group_std  # (B, G)\n","    advantages = advantages_group.view(-1)  # (B*G,)\n","\n","    # 7) Policy gradient loss (+ optional entropy bonus)\n","    pg_loss = -(advantages.detach() * logprob_seq).mean()\n","\n","    if entropy_coef > 0.0:\n","        probs = torch.softmax(logits[:, :-1, :], dim=-1)\n","        entropy_tokens = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)  # (B*G, L-1)\n","        entropy_seq = (entropy_tokens * final_mask).sum(dim=1) / final_mask.sum(dim=1).clamp_min(1)\n","        loss = pg_loss - entropy_coef * entropy_seq.mean()\n","    else:\n","        loss = pg_loss\n","\n","    return loss, advantages, task_rewards, kl_seq\n","\n","\n","# --- sanity test (no optimizer step) ---\n","BATCH_SIZE = 2\n","G = 2\n","\n","batch_prompts = [d[\"prompt\"] for d in train_data[:BATCH_SIZE]]\n","batch_labels  = [d[\"label\"]  for d in train_data[:BATCH_SIZE]]\n","\n","model.eval()\n","loss, advantages, task_rewards, kl_seq = grpo_step(\n","    batch_prompts,\n","    batch_labels,\n","    G=G,\n","    max_new_tokens=MAX_LABEL_SEQ_LEN + 1,\n","    beta_kl=0.0,\n","    use_kl=False,\n","    entropy_coef=0.0\n",")\n","\n","print(\"Advantages shape:\", advantages.shape)\n","print(\"Initial loss:\", loss.detach().item())\n","print(\"Task rewards shape:\", task_rewards.shape)\n","print(\"KL values shape:\", kl_seq.shape)\n","print(\"Sanity completions (first few):\")\n","# quick check to ensure constrained labels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBHGxcq4aMkr","executionInfo":{"status":"ok","timestamp":1767347250465,"user_tz":300,"elapsed":2100,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"7b0e0a0e-b6de-4d47-f977-667114a672f5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Advantages shape: torch.Size([4])\n","Initial loss: -0.0\n","Task rewards shape: torch.Size([4])\n","KL values shape: torch.Size([4])\n","Sanity completions (first few):\n"]}]},{"cell_type":"code","source":["# Cell 6: GRPO Training Loop (CPU + KL)\n","\n","import random\n","import torch\n","\n","device = next(model.parameters()).device\n","print(\"Training on device:\", device)\n","\n","learning_rate = 1e-5\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","num_steps = 100       # what you used before\n","batch_size = 4\n","G = 4\n","log_interval = 10\n","\n","#beta_kl = 0.1        # <-- KL weight\n","#use_kl = True        # <-- ENABLE KL\n","beta_kl=0.0\n","use_kl=False\n","\n","running_loss = 0.0\n","running_reward = 0.0\n","running_kl = 0.0\n","\n","#print(\"Starting GRPO training with KL (CPU)...\\n\")\n","print(\"Starting GRPO training without KL (CPU)...\\n\")\n","\n","for step in range(1, num_steps + 1):\n","    batch = random.sample(train_data, batch_size)\n","    batch_prompts = [d[\"prompt\"] for d in batch]\n","    batch_labels  = [d[\"label\"]  for d in batch]\n","\n","    model.train()\n","\n","    loss, advantages, task_rewards, kl_seq = grpo_step(\n","        batch_prompts,\n","        batch_labels,\n","        G=G,\n","        max_new_tokens=8,\n","        beta_kl=beta_kl,\n","        use_kl=use_kl,\n","    )\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","\n","    avg_reward = task_rewards.mean().detach().item()\n","    avg_kl = kl_seq.mean().detach().item() if use_kl else 0.0\n","\n","    running_loss += loss.detach().item()\n","    running_reward += avg_reward\n","    running_kl += avg_kl\n","\n","    if step % log_interval == 0: #c\n","        print( # KL stage is commented out in this version\n","            f\"Step {step:3d}/{num_steps} | \"\n","            f\"Avg Loss: {running_loss / log_interval:.4f} | \"\n","            f\"Avg Reward: {running_reward / log_interval:.4f}\"\n","            #f\" | Avg KL: {running_kl / log_interval:.4f}\"\n","        )\n","        running_loss = 0.0\n","        running_reward = 0.0\n","        #running_kl = 0.0\n","\n","print(\"\\nTraining complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8AFmZACcLGh","executionInfo":{"status":"ok","timestamp":1767348057317,"user_tz":300,"elapsed":806846,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"cd811fed-ac70-47b9-f6f6-611337bd34f3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on device: cpu\n","Starting GRPO training without KL (CPU)...\n","\n","Step  10/100 | Avg Loss: -0.8482 | Avg Reward: 0.1094\n","Step  20/100 | Avg Loss: -0.4865 | Avg Reward: 0.1719\n","Step  30/100 | Avg Loss: -0.3979 | Avg Reward: 0.0938\n","Step  40/100 | Avg Loss: -0.1087 | Avg Reward: 0.1437\n","Step  50/100 | Avg Loss: 0.0000 | Avg Reward: 0.2000\n","Step  60/100 | Avg Loss: -0.0718 | Avg Reward: 0.1187\n","Step  70/100 | Avg Loss: -0.2898 | Avg Reward: 0.2375\n","Step  80/100 | Avg Loss: 0.0000 | Avg Reward: 0.1000\n","Step  90/100 | Avg Loss: 0.0000 | Avg Reward: 0.2000\n","Step 100/100 | Avg Loss: 0.0000 | Avg Reward: 0.1750\n","\n","Training complete.\n"]}]},{"cell_type":"code","source":["# Cell 7: Evaluation and Generation (with constrained decoding + robust completion slicing)\n","\n","import torch\n","\n","# --- helpers for constrained decoding (must match Cell 5) ---\n","# Assumes ALL_LABEL_SEQS, MAX_LABEL_SEQ_LEN are defined in Cell 5\n","\n","def make_prefix_allowed_tokens_fn(input_len, allowed_label_seqs, eos_id, max_label_seq_len):\n","    \"\"\"\n","    input_len: padded input length at generation start (int)\n","    allowed_label_seqs: list[list[int]] tokenized labels (e.g., ALL_LABEL_SEQS)\n","    \"\"\"\n","    def prefix_allowed_tokens_fn(batch_id, input_ids):\n","        # Number of tokens generated so far beyond the original (padded) prompt\n","        gen_len = input_ids.shape[0] - input_len\n","\n","        candidates = set()\n","        if gen_len < 0:\n","            return [eos_id]\n","\n","        if gen_len < max_label_seq_len:\n","            for seq in allowed_label_seqs:\n","                if gen_len < len(seq):\n","                    candidates.add(seq[gen_len])\n","                elif gen_len == len(seq):\n","                    candidates.add(eos_id)\n","        else:\n","            candidates.add(eos_id)\n","\n","        return list(candidates)\n","\n","    return prefix_allowed_tokens_fn\n","\n","\n","def evaluate_model(model_to_eval, dataset, num_samples=200, batch_size=8):\n","    \"\"\"\n","    Deterministic evaluation with constrained decoding so the model can only output one of the labels.\n","    \"\"\"\n","    model_to_eval.eval()\n","    device = next(model_to_eval.parameters()).device\n","\n","    num_samples = min(num_samples, len(dataset))\n","    eval_subset = dataset[:num_samples]\n","\n","    all_rewards = []\n","\n","    with torch.no_grad():\n","        for start in range(0, num_samples, batch_size):\n","            batch = eval_subset[start:start + batch_size]\n","            prompts = [d[\"prompt\"] for d in batch]\n","            labels  = [d[\"label\"]  for d in batch]\n","\n","            enc = tokenizer(\n","                prompts,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True\n","            ).to(device)\n","\n","            input_ids = enc[\"input_ids\"]\n","            attention_mask = enc[\"attention_mask\"]\n","\n","            # Use padded input length for correct gen_len tracking under left padding\n","            input_len = input_ids.shape[1]\n","            prefix_fn = make_prefix_allowed_tokens_fn(\n","                input_len=input_len,\n","                allowed_label_seqs=ALL_LABEL_SEQS,\n","                eos_id=tokenizer.eos_token_id,\n","                max_label_seq_len=MAX_LABEL_SEQ_LEN\n","            )\n","\n","            gen_outputs = model_to_eval.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                max_new_tokens=MAX_LABEL_SEQ_LEN + 1,   # enough for full label + EOS\n","                do_sample=False,                        # deterministic eval\n","                prefix_allowed_tokens_fn=prefix_fn,      # <-- constraint ON\n","                pad_token_id=tokenizer.pad_token_id,\n","                eos_token_id=tokenizer.eos_token_id,\n","                return_dict_in_generate=True,\n","            )\n","            gen_sequences = gen_outputs.sequences\n","\n","            # Decode ONLY the completion part (after the padded input)\n","            completions = []\n","            for seq in gen_sequences:\n","                completion_ids = seq[input_len:]  # slice after padded input length\n","                text = tokenizer.decode(completion_ids, skip_special_tokens=True)\n","                completions.append(text)\n","\n","            rewards = get_reward(prompts, completions, labels)\n","            all_rewards.append(rewards)\n","\n","    all_rewards = torch.cat(all_rewards, dim=0)\n","    return all_rewards.mean().item()\n","\n","\n","# ---------------------------------------------\n","# Quantitative Evaluation\n","# ---------------------------------------------\n","print(\"Running quantitative evaluation...\\n\")\n","\n","baseline_reward = evaluate_model(reference_model, eval_data, num_samples=200, batch_size=8)\n","finetuned_reward = evaluate_model(model, eval_data, num_samples=200, batch_size=8)\n","\n","print(f\"Baseline mean reward (reference model): {baseline_reward:.4f}\")\n","print(f\"Fine-tuned mean reward (policy model):  {finetuned_reward:.4f}\")\n","\n","# ---------------------------------------------\n","# Qualitative Examples (Fine-tuned model)\n","# ---------------------------------------------\n","print(\"\\nQualitative examples from fine-tuned model:\\n\")\n","\n","device = next(model.parameters()).device\n","model.eval()\n","\n","num_examples = 5\n","example_batch = eval_data[:num_examples]\n","\n","with torch.no_grad():\n","    prompts = [d[\"prompt\"] for d in example_batch]\n","    labels  = [d[\"label\"]  for d in example_batch]\n","\n","    enc = tokenizer(\n","        prompts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True\n","    ).to(device)\n","\n","    input_ids = enc[\"input_ids\"]\n","    attention_mask = enc[\"attention_mask\"]\n","\n","    input_len = input_ids.shape[1]\n","    prefix_fn = make_prefix_allowed_tokens_fn(\n","        input_len=input_len,\n","        allowed_label_seqs=ALL_LABEL_SEQS,\n","        eos_id=tokenizer.eos_token_id,\n","        max_label_seq_len=MAX_LABEL_SEQ_LEN\n","    )\n","\n","    gen_outputs = model.generate(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        max_new_tokens=MAX_LABEL_SEQ_LEN + 1,\n","        do_sample=False,\n","        prefix_allowed_tokens_fn=prefix_fn,   # <-- constraint ON\n","        pad_token_id=tokenizer.pad_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","        return_dict_in_generate=True,\n","    )\n","    gen_sequences = gen_outputs.sequences\n","\n","    completions = []\n","    for seq in gen_sequences:\n","        completion_ids = seq[input_len:]\n","        text = tokenizer.decode(completion_ids, skip_special_tokens=True)\n","        completions.append(text)\n","\n","    rewards = get_reward(prompts, completions, labels)\n","\n","for i, (ex, out, r) in enumerate(zip(example_batch, completions, rewards)):\n","    print(f\"Example {i+1}:\")\n","    print(\"Prompt:\")\n","    print(ex[\"prompt\"])\n","    print(\"\\nModel Output:\")\n","    print(repr(out))\n","    print(\"Gold Label:\", ex[\"label\"])\n","    print(\"Reward:\", float(r))\n","    print(\"-\" * 60)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_tTt5BZjqxp","executionInfo":{"status":"ok","timestamp":1767348113223,"user_tz":300,"elapsed":55908,"user":{"displayName":"StarSword C","userId":"06010562867723972428"}},"outputId":"984811c6-d76a-4393-a73d-9fa69bf90464"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Running quantitative evaluation...\n","\n","Baseline mean reward (reference model): 0.0000\n","Fine-tuned mean reward (policy model):  0.2500\n","\n","Qualitative examples from fine-tuned model:\n","\n","Example 1:\n","Prompt:\n","Classify this 5-card poker hand.\n","\n","Hand: 9H KC AD TD 6S\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","\n","Model Output:\n","' high card'\n","Gold Label: high card\n","Reward: 1.0\n","------------------------------------------------------------\n","Example 2:\n","Prompt:\n","Classify this 5-card poker hand.\n","\n","Hand: AC QC 5D 8S 5C\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","\n","Model Output:\n","' high card'\n","Gold Label: one pair\n","Reward: 0.0\n","------------------------------------------------------------\n","Example 3:\n","Prompt:\n","Classify this 5-card poker hand.\n","\n","Hand: 6S 5C TS KH JC\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","\n","Model Output:\n","' high card'\n","Gold Label: high card\n","Reward: 1.0\n","------------------------------------------------------------\n","Example 4:\n","Prompt:\n","Classify this 5-card poker hand.\n","\n","Hand: 2H 4H 8S 2C JC\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","\n","Model Output:\n","' high card'\n","Gold Label: one pair\n","Reward: 0.0\n","------------------------------------------------------------\n","Example 5:\n","Prompt:\n","Classify this 5-card poker hand.\n","\n","Hand: TH 8C QD 7H KS\n","Question: What type of hand is this? Answer with exactly one of:\n","high card, one pair, two pair, three of a kind, straight, flush, full house, four of a kind.\n","Do not explain your answer.\n","\n","Answer:\n","\n","Model Output:\n","' high card'\n","Gold Label: high card\n","Reward: 1.0\n","------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Cell 8: Analysis\n","\n","This section summarizes the results from **two versions** of the experiment:\n","\n","- **Version 1:** Initial implementation (imbalanced dataset, reward bug, weak RL signal).  \n","- **Version 2:** Corrected and improved implementation (balanced dataset, fixed reward logic, reward shaping).\n","\n","---\n","\n","## Version 1: Initial GRPO Setup\n","\n","### Results Summary\n","- **Baseline reward:** ~0.53  \n","- **Fine-tuned reward:** ~0.53  \n","- **Qualitative behavior:** The model frequently responded with explanatory text such as  \n","  *“A high card is a hand that has…”*, which contains the substring `\"high card\"`.\n","\n","### Interpretation\n","At first glance, the baseline performance (~0.53 accuracy) seemed surprisingly high. However, this version had two core issues:\n","\n","1. **Class imbalance** in the training data:  \n","   Over half of all possible 5-card hands are **high card**, so a strategy of always predicting `\"high card\"` yields strong reward under a uniform random sampling of hands.\n","\n","2. **Reward computation bug:**  \n","   The reward extractor (`extract_label`) mistakenly scanned the **entire decoded sequence**, including the **prompt**, not just the model’s completion.  \n","   Because the prompt itself contained the list of labels (including `\"high card\"`), the reward function frequently detected `\"high card\"` even when the model’s actual output contained no valid label at all.\n","\n","As a result:\n","- The model appeared to perform well, but was actually receiving credit independent of its generated answers.\n","- GRPO training had **no effect**, because the model’s actions did not meaningfully influence the reward.\n","\n","This version revealed an important lesson:  \n","**RL on language models is extremely sensitive to how rewards are extracted from model outputs.**\n","\n","---\n","\n","## Version 2: Corrected and Improved GRPO Setup\n","\n","After identifying the issues above, we created a second fork of the notebook with three major improvements:\n","\n","1. **Balanced Dataset**  \n","   The toy dataset was regenerated to include equal numbers of all eight hand types.  \n","   This removes the incentive to always predict `\"high card\"`.\n","\n","2. **Reward Shaping**  \n","   Harder-to-identify hands (e.g., straight, full house) received higher reward values, encouraging the model to learn beyond trivial classes.\n","\n","3. **Correct Reward Isolation**  \n","   Only the **completion tokens** (what the model actually generated after `Answer:`) were decoded and scored.  \n","   This fixed the problem where the prompt itself was being rewarded.\n","\n","---\n","\n","## Version 3: KL-Regularized GRPO (CPU)\n","\n","In a third experiment (shown), I enabled **KL regularization** in the GRPO loop using a frozen reference model:\n","\n","- `use_kl = True`\n","- `beta_kl = 0.1` (KL weight)\n","- Same balanced dataset and reward shaping as Version 2\n","- Same training budget (100 steps, batch size = 4, group size \\(G = 4\\))\n","\n","### Results Summary\n","\n","- **Baseline reward (reference model):** ~0.03  \n","- **Fine-tuned reward (KL-regularized policy):** ~0.04  \n","\n","Training logs showed small but nonzero KL values (on the order of $\\pm 0.02–0.03$) and average rewards that fluctuated around a low value (typically \\(0.03\\)–\\(0.09\\) per logging window).\n","\n","Qualitative examples indicate that the KL-regularized policy often produces incomplete or generic continuations such as:\n","\n","- Fragments of the hand (e.g., `\"5S QD 2H\"`, `\": 3D JC 2C\"`)\n","- Generic phrases (e.g., `\"Answer: The 5-card poker hand is\"`)\n","- Occasional incorrect labels (e.g., `\"A straight.\"` for a high-card hand)\n","\n","In other words, with KL turned on, the model’s behavior remains much closer to the **untrained reference model** and rarely produces clean, task-specific labels.\n","\n","### Interpretation\n","\n","This run illustrates the **trade-off introduced by KL regularization**:\n","\n","- The reference model is **poor at the task** (reward ~0.03) and rarely outputs valid labels.\n","- KL regularization explicitly penalizes deviations from this reference behavior.\n","- With a **small RL budget** (100 steps, tiny batches on CPU), the GRPO updates do not have enough strength to both:\n","  - escape the reference model’s poor behavior, **and**\n","  - improve reward significantly.\n","\n","As a result, the KL-regularized policy remains only marginally better than the baseline, and far worse than the **non-KL Version 2**, which reached a mean reward of ~0.25 by allowing the model to move more freely away from the reference distribution.\n","\n","### Takeaways from All Three Versions\n","\n","Across the three versions, the experiments highlight:\n","\n","1. **Version 1 (imbalanced + buggy reward):**  \n","   Apparent high performance can be completely spurious if the reward function accidentally scores the prompt rather than the completion.\n","\n","2. **Version 2 (balanced + fixed reward, no KL):**  \n","   With a corrected setup, GRPO can significantly improve task-specific behavior (mean reward ~0.25) even with limited compute, at the cost of drifting away from the base model.\n","\n","3. **Version 3 (balanced + fixed reward, with KL):**  \n","   Adding KL regularization stabilizes the policy around the reference model but, given the small RL budget and a weak reference policy, it also **limits how much improvement is possible**, yielding only a small gain over baseline.\n","\n","This mirrors real-world RLHF trade-offs:  \n","KL helps prevent catastrophic drift and preserves general language ability, but if overemphasized or combined with a weak reference policy and limited RL signal, it can substantially blunt the benefit of task-specific fine-tuning."],"metadata":{"id":"94K9n_qur7uM"}}]}